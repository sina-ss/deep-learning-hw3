{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b9bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Attempting to load local MNIST dataset...\n",
      "Successfully loaded local dataset: 60000 training samples\n",
      "Dataset loaded: 60000 training samples\n",
      "Networks initialized\n",
      "Generator parameters: 1489936\n",
      "Discriminator parameters: 566273\n",
      "Starting training...\n",
      "Epoch [1/50] Batch [0/938] D_loss: 1.3795 G_loss: 0.6926 D_real_acc: 0.9219 D_fake_acc: 0.3438\n",
      "Epoch [1/50] Batch [100/938] D_loss: 1.3003 G_loss: 0.9508 D_real_acc: 0.5156 D_fake_acc: 0.8281\n",
      "Epoch [1/50] Batch [200/938] D_loss: 1.5763 G_loss: 0.5817 D_real_acc: 0.5000 D_fake_acc: 0.1094\n",
      "Epoch [1/50] Batch [300/938] D_loss: 1.2491 G_loss: 0.7461 D_real_acc: 0.7500 D_fake_acc: 0.6406\n",
      "Epoch [1/50] Batch [400/938] D_loss: 1.2005 G_loss: 0.9441 D_real_acc: 0.7031 D_fake_acc: 0.7656\n",
      "Epoch [1/50] Batch [500/938] D_loss: 1.1927 G_loss: 0.6048 D_real_acc: 0.4375 D_fake_acc: 0.9688\n",
      "Epoch [1/50] Batch [600/938] D_loss: 1.2234 G_loss: 1.3211 D_real_acc: 0.9219 D_fake_acc: 0.3906\n",
      "Epoch [1/50] Batch [700/938] D_loss: 1.1460 G_loss: 1.3297 D_real_acc: 0.8594 D_fake_acc: 0.5156\n",
      "Epoch [1/50] Batch [800/938] D_loss: 1.2893 G_loss: 1.0803 D_real_acc: 0.7031 D_fake_acc: 0.5469\n",
      "Epoch [1/50] Batch [900/938] D_loss: 1.1829 G_loss: 0.9657 D_real_acc: 0.6875 D_fake_acc: 0.6250\n",
      "Epoch [1/50] Average - D_loss: 1.2278 G_loss: 0.9842 D_real_acc: 0.7604 D_fake_acc: 0.5298\n",
      "Generated images saved to generated_images/epoch_01.png\n",
      "Epoch [2/50] Batch [0/938] D_loss: 1.0832 G_loss: 1.0481 D_real_acc: 0.6094 D_fake_acc: 0.9219\n",
      "Epoch [2/50] Batch [100/938] D_loss: 1.2285 G_loss: 1.5040 D_real_acc: 0.8906 D_fake_acc: 0.3594\n",
      "Epoch [2/50] Batch [200/938] D_loss: 1.2061 G_loss: 1.0747 D_real_acc: 0.8594 D_fake_acc: 0.4844\n",
      "Epoch [2/50] Batch [300/938] D_loss: 1.2322 G_loss: 1.1015 D_real_acc: 0.5781 D_fake_acc: 0.7812\n",
      "Epoch [2/50] Batch [400/938] D_loss: 1.0106 G_loss: 0.9662 D_real_acc: 0.6875 D_fake_acc: 0.7969\n",
      "Epoch [2/50] Batch [500/938] D_loss: 1.1730 G_loss: 1.1560 D_real_acc: 0.5312 D_fake_acc: 0.7188\n",
      "Epoch [2/50] Batch [600/938] D_loss: 1.0362 G_loss: 1.0744 D_real_acc: 0.6875 D_fake_acc: 0.8594\n",
      "Epoch [2/50] Batch [700/938] D_loss: 1.1270 G_loss: 0.9265 D_real_acc: 0.3906 D_fake_acc: 0.9531\n",
      "Epoch [2/50] Batch [800/938] D_loss: 1.2524 G_loss: 1.0813 D_real_acc: 0.3281 D_fake_acc: 0.9375\n",
      "Epoch [2/50] Batch [900/938] D_loss: 1.2036 G_loss: 0.9216 D_real_acc: 0.6094 D_fake_acc: 0.7031\n",
      "Epoch [2/50] Average - D_loss: 1.1981 G_loss: 1.0787 D_real_acc: 0.6158 D_fake_acc: 0.7303\n",
      "Epoch [3/50] Batch [0/938] D_loss: 1.3271 G_loss: 1.5682 D_real_acc: 0.8281 D_fake_acc: 0.2969\n",
      "Epoch [3/50] Batch [100/938] D_loss: 1.2428 G_loss: 1.1448 D_real_acc: 0.6094 D_fake_acc: 0.5469\n",
      "Epoch [3/50] Batch [200/938] D_loss: 1.2812 G_loss: 1.1646 D_real_acc: 0.7188 D_fake_acc: 0.4688\n",
      "Epoch [3/50] Batch [300/938] D_loss: 1.2742 G_loss: 0.8472 D_real_acc: 0.4219 D_fake_acc: 0.7969\n",
      "Epoch [3/50] Batch [400/938] D_loss: 1.1518 G_loss: 0.9372 D_real_acc: 0.7812 D_fake_acc: 0.7188\n",
      "Epoch [3/50] Batch [500/938] D_loss: 1.2745 G_loss: 0.7910 D_real_acc: 0.5781 D_fake_acc: 0.6719\n",
      "Epoch [3/50] Batch [600/938] D_loss: 1.2703 G_loss: 1.2319 D_real_acc: 0.7812 D_fake_acc: 0.4844\n",
      "Epoch [3/50] Batch [700/938] D_loss: 1.1309 G_loss: 0.9556 D_real_acc: 0.6406 D_fake_acc: 0.8281\n",
      "Epoch [3/50] Batch [800/938] D_loss: 1.3056 G_loss: 0.8233 D_real_acc: 0.5625 D_fake_acc: 0.7188\n",
      "Epoch [3/50] Batch [900/938] D_loss: 1.2341 G_loss: 0.9134 D_real_acc: 0.6719 D_fake_acc: 0.7031\n",
      "Epoch [3/50] Average - D_loss: 1.2467 G_loss: 0.9531 D_real_acc: 0.5753 D_fake_acc: 0.7181\n",
      "Epoch [4/50] Batch [0/938] D_loss: 1.2654 G_loss: 0.7804 D_real_acc: 0.5156 D_fake_acc: 0.7812\n",
      "Epoch [4/50] Batch [100/938] D_loss: 1.2339 G_loss: 0.5701 D_real_acc: 0.5156 D_fake_acc: 0.8125\n",
      "Epoch [4/50] Batch [200/938] D_loss: 1.2954 G_loss: 0.7858 D_real_acc: 0.4844 D_fake_acc: 0.7812\n",
      "Epoch [4/50] Batch [300/938] D_loss: 1.3680 G_loss: 0.9315 D_real_acc: 0.4375 D_fake_acc: 0.7969\n",
      "Epoch [4/50] Batch [400/938] D_loss: 1.3044 G_loss: 0.9520 D_real_acc: 0.3281 D_fake_acc: 0.9062\n",
      "Epoch [4/50] Batch [500/938] D_loss: 1.2414 G_loss: 0.7211 D_real_acc: 0.5781 D_fake_acc: 0.6562\n",
      "Epoch [4/50] Batch [600/938] D_loss: 1.2372 G_loss: 1.0816 D_real_acc: 0.7812 D_fake_acc: 0.4688\n",
      "Epoch [4/50] Batch [700/938] D_loss: 1.2983 G_loss: 0.7767 D_real_acc: 0.5781 D_fake_acc: 0.6875\n",
      "Epoch [4/50] Batch [800/938] D_loss: 1.2926 G_loss: 0.9556 D_real_acc: 0.3281 D_fake_acc: 0.9062\n",
      "Epoch [4/50] Batch [900/938] D_loss: 1.2579 G_loss: 0.9610 D_real_acc: 0.8438 D_fake_acc: 0.4844\n",
      "Epoch [4/50] Average - D_loss: 1.2754 G_loss: 0.8955 D_real_acc: 0.5510 D_fake_acc: 0.7108\n",
      "Epoch [5/50] Batch [0/938] D_loss: 1.2414 G_loss: 0.8432 D_real_acc: 0.5469 D_fake_acc: 0.7031\n",
      "Epoch [5/50] Batch [100/938] D_loss: 1.2600 G_loss: 1.0000 D_real_acc: 0.6562 D_fake_acc: 0.6250\n",
      "Epoch [5/50] Batch [200/938] D_loss: 1.2381 G_loss: 0.8725 D_real_acc: 0.5781 D_fake_acc: 0.6875\n",
      "Epoch [5/50] Batch [300/938] D_loss: 1.1939 G_loss: 0.8652 D_real_acc: 0.5469 D_fake_acc: 0.7969\n",
      "Epoch [5/50] Batch [400/938] D_loss: 1.3975 G_loss: 0.9031 D_real_acc: 0.3594 D_fake_acc: 0.6406\n",
      "Epoch [5/50] Batch [500/938] D_loss: 1.3682 G_loss: 0.8295 D_real_acc: 0.5156 D_fake_acc: 0.7500\n",
      "Epoch [5/50] Batch [600/938] D_loss: 1.1636 G_loss: 0.9176 D_real_acc: 0.6250 D_fake_acc: 0.7812\n",
      "Epoch [5/50] Batch [700/938] D_loss: 1.3373 G_loss: 0.8188 D_real_acc: 0.5781 D_fake_acc: 0.6562\n",
      "Epoch [5/50] Batch [800/938] D_loss: 1.3543 G_loss: 0.8550 D_real_acc: 0.5000 D_fake_acc: 0.6562\n",
      "Epoch [5/50] Batch [900/938] D_loss: 1.3313 G_loss: 0.7465 D_real_acc: 0.4531 D_fake_acc: 0.7656\n",
      "Epoch [5/50] Average - D_loss: 1.2977 G_loss: 0.8552 D_real_acc: 0.5294 D_fake_acc: 0.7032\n",
      "Generated images saved to generated_images/epoch_05.png\n",
      "Epoch [6/50] Batch [0/938] D_loss: 1.3295 G_loss: 0.8997 D_real_acc: 0.2969 D_fake_acc: 0.8438\n",
      "Epoch [6/50] Batch [100/938] D_loss: 1.2971 G_loss: 0.8439 D_real_acc: 0.5625 D_fake_acc: 0.6719\n",
      "Epoch [6/50] Batch [200/938] D_loss: 1.3591 G_loss: 0.8280 D_real_acc: 0.4844 D_fake_acc: 0.5938\n",
      "Epoch [6/50] Batch [300/938] D_loss: 1.3390 G_loss: 0.7429 D_real_acc: 0.6562 D_fake_acc: 0.5625\n",
      "Epoch [6/50] Batch [400/938] D_loss: 1.2950 G_loss: 0.8060 D_real_acc: 0.4844 D_fake_acc: 0.7812\n",
      "Epoch [6/50] Batch [500/938] D_loss: 1.2346 G_loss: 0.8654 D_real_acc: 0.6719 D_fake_acc: 0.6250\n",
      "Epoch [6/50] Batch [600/938] D_loss: 1.3291 G_loss: 0.7455 D_real_acc: 0.4844 D_fake_acc: 0.7031\n",
      "Epoch [6/50] Batch [700/938] D_loss: 1.3656 G_loss: 0.8274 D_real_acc: 0.4844 D_fake_acc: 0.5469\n",
      "Epoch [6/50] Batch [800/938] D_loss: 1.2815 G_loss: 0.7704 D_real_acc: 0.3750 D_fake_acc: 0.7969\n",
      "Epoch [6/50] Batch [900/938] D_loss: 1.2720 G_loss: 0.7919 D_real_acc: 0.6562 D_fake_acc: 0.6250\n",
      "Epoch [6/50] Average - D_loss: 1.3228 G_loss: 0.8140 D_real_acc: 0.5019 D_fake_acc: 0.6920\n",
      "Epoch [7/50] Batch [0/938] D_loss: 1.3592 G_loss: 0.8114 D_real_acc: 0.3281 D_fake_acc: 0.7969\n",
      "Epoch [7/50] Batch [100/938] D_loss: 1.3401 G_loss: 0.8039 D_real_acc: 0.4688 D_fake_acc: 0.7031\n",
      "Epoch [7/50] Batch [200/938] D_loss: 1.3834 G_loss: 0.8768 D_real_acc: 0.3750 D_fake_acc: 0.7344\n",
      "Epoch [7/50] Batch [300/938] D_loss: 1.3154 G_loss: 0.7910 D_real_acc: 0.6094 D_fake_acc: 0.5625\n",
      "Epoch [7/50] Batch [400/938] D_loss: 1.3981 G_loss: 0.8441 D_real_acc: 0.5312 D_fake_acc: 0.5312\n",
      "Epoch [7/50] Batch [500/938] D_loss: 1.3121 G_loss: 0.8229 D_real_acc: 0.4844 D_fake_acc: 0.7344\n",
      "Epoch [7/50] Batch [600/938] D_loss: 1.2561 G_loss: 0.8568 D_real_acc: 0.7500 D_fake_acc: 0.5156\n",
      "Epoch [7/50] Batch [700/938] D_loss: 1.2475 G_loss: 0.7683 D_real_acc: 0.6250 D_fake_acc: 0.5781\n",
      "Epoch [7/50] Batch [800/938] D_loss: 1.3492 G_loss: 0.8063 D_real_acc: 0.5312 D_fake_acc: 0.6250\n",
      "Epoch [7/50] Batch [900/938] D_loss: 1.3411 G_loss: 0.7654 D_real_acc: 0.3750 D_fake_acc: 0.8281\n",
      "Epoch [7/50] Average - D_loss: 1.3342 G_loss: 0.7992 D_real_acc: 0.4980 D_fake_acc: 0.6825\n",
      "Epoch [8/50] Batch [0/938] D_loss: 1.2882 G_loss: 0.8004 D_real_acc: 0.6094 D_fake_acc: 0.6875\n",
      "Epoch [8/50] Batch [100/938] D_loss: 1.2961 G_loss: 0.6949 D_real_acc: 0.6250 D_fake_acc: 0.6875\n",
      "Epoch [8/50] Batch [200/938] D_loss: 1.3692 G_loss: 0.7612 D_real_acc: 0.4688 D_fake_acc: 0.6719\n",
      "Epoch [8/50] Batch [300/938] D_loss: 1.3498 G_loss: 0.7954 D_real_acc: 0.4219 D_fake_acc: 0.6250\n",
      "Epoch [8/50] Batch [400/938] D_loss: 1.4175 G_loss: 0.7781 D_real_acc: 0.4844 D_fake_acc: 0.5469\n",
      "Epoch [8/50] Batch [500/938] D_loss: 1.3949 G_loss: 0.8149 D_real_acc: 0.3125 D_fake_acc: 0.7969\n",
      "Epoch [8/50] Batch [600/938] D_loss: 1.3861 G_loss: 0.7414 D_real_acc: 0.4062 D_fake_acc: 0.7031\n",
      "Epoch [8/50] Batch [700/938] D_loss: 1.3250 G_loss: 0.7504 D_real_acc: 0.4375 D_fake_acc: 0.7031\n",
      "Epoch [8/50] Batch [800/938] D_loss: 1.3442 G_loss: 0.7288 D_real_acc: 0.6875 D_fake_acc: 0.4375\n",
      "Epoch [8/50] Batch [900/938] D_loss: 1.3031 G_loss: 0.7389 D_real_acc: 0.8125 D_fake_acc: 0.4219\n",
      "Epoch [8/50] Average - D_loss: 1.3459 G_loss: 0.7758 D_real_acc: 0.4858 D_fake_acc: 0.6693\n",
      "Epoch [9/50] Batch [0/938] D_loss: 1.3880 G_loss: 0.8633 D_real_acc: 0.1406 D_fake_acc: 0.8906\n",
      "Epoch [9/50] Batch [100/938] D_loss: 1.3201 G_loss: 0.7916 D_real_acc: 0.5156 D_fake_acc: 0.7656\n",
      "Epoch [9/50] Batch [200/938] D_loss: 1.3896 G_loss: 0.7508 D_real_acc: 0.3281 D_fake_acc: 0.7500\n",
      "Epoch [9/50] Batch [300/938] D_loss: 1.3667 G_loss: 0.7949 D_real_acc: 0.2500 D_fake_acc: 0.8125\n",
      "Epoch [9/50] Batch [400/938] D_loss: 1.3966 G_loss: 0.7494 D_real_acc: 0.4531 D_fake_acc: 0.5938\n",
      "Epoch [9/50] Batch [500/938] D_loss: 1.4083 G_loss: 0.8133 D_real_acc: 0.2500 D_fake_acc: 0.6562\n",
      "Epoch [9/50] Batch [600/938] D_loss: 1.3051 G_loss: 0.7304 D_real_acc: 0.6094 D_fake_acc: 0.6094\n",
      "Epoch [9/50] Batch [700/938] D_loss: 1.3234 G_loss: 0.7305 D_real_acc: 0.6094 D_fake_acc: 0.6094\n",
      "Epoch [9/50] Batch [800/938] D_loss: 1.3368 G_loss: 0.6988 D_real_acc: 0.7812 D_fake_acc: 0.4062\n",
      "Epoch [9/50] Batch [900/938] D_loss: 1.4190 G_loss: 0.7762 D_real_acc: 0.3750 D_fake_acc: 0.6094\n",
      "Epoch [9/50] Average - D_loss: 1.3575 G_loss: 0.7582 D_real_acc: 0.4751 D_fake_acc: 0.6554\n",
      "Epoch [10/50] Batch [0/938] D_loss: 1.3955 G_loss: 0.7535 D_real_acc: 0.2188 D_fake_acc: 0.8125\n",
      "Epoch [10/50] Batch [100/938] D_loss: 1.3732 G_loss: 0.7463 D_real_acc: 0.4375 D_fake_acc: 0.7344\n",
      "Epoch [10/50] Batch [200/938] D_loss: 1.3679 G_loss: 0.7321 D_real_acc: 0.5000 D_fake_acc: 0.6719\n",
      "Epoch [10/50] Batch [300/938] D_loss: 1.3598 G_loss: 0.7595 D_real_acc: 0.3906 D_fake_acc: 0.6875\n",
      "Epoch [10/50] Batch [400/938] D_loss: 1.3693 G_loss: 0.7717 D_real_acc: 0.3906 D_fake_acc: 0.7031\n",
      "Epoch [10/50] Batch [500/938] D_loss: 1.3670 G_loss: 0.7480 D_real_acc: 0.4688 D_fake_acc: 0.6875\n",
      "Epoch [10/50] Batch [600/938] D_loss: 1.3956 G_loss: 0.8240 D_real_acc: 0.2188 D_fake_acc: 0.7812\n",
      "Epoch [10/50] Batch [700/938] D_loss: 1.4088 G_loss: 0.7572 D_real_acc: 0.3750 D_fake_acc: 0.6094\n",
      "Epoch [10/50] Batch [800/938] D_loss: 1.3915 G_loss: 0.8054 D_real_acc: 0.3125 D_fake_acc: 0.6250\n",
      "Epoch [10/50] Batch [900/938] D_loss: 1.3867 G_loss: 0.7621 D_real_acc: 0.5625 D_fake_acc: 0.4531\n",
      "Epoch [10/50] Average - D_loss: 1.3605 G_loss: 0.7507 D_real_acc: 0.4631 D_fake_acc: 0.6583\n",
      "Generated images saved to generated_images/epoch_10.png\n",
      "Epoch [11/50] Batch [0/938] D_loss: 1.3731 G_loss: 0.6818 D_real_acc: 0.6562 D_fake_acc: 0.4375\n",
      "Epoch [11/50] Batch [100/938] D_loss: 1.3360 G_loss: 0.7511 D_real_acc: 0.5312 D_fake_acc: 0.6875\n",
      "Epoch [11/50] Batch [200/938] D_loss: 1.3808 G_loss: 0.7640 D_real_acc: 0.3125 D_fake_acc: 0.7344\n",
      "Epoch [11/50] Batch [300/938] D_loss: 1.3954 G_loss: 0.7820 D_real_acc: 0.1094 D_fake_acc: 0.8750\n",
      "Epoch [11/50] Batch [400/938] D_loss: 1.3867 G_loss: 0.7190 D_real_acc: 0.3594 D_fake_acc: 0.5781\n",
      "Epoch [11/50] Batch [500/938] D_loss: 1.3799 G_loss: 0.7669 D_real_acc: 0.5469 D_fake_acc: 0.5781\n",
      "Epoch [11/50] Batch [600/938] D_loss: 1.3338 G_loss: 0.7154 D_real_acc: 0.4844 D_fake_acc: 0.7656\n",
      "Epoch [11/50] Batch [700/938] D_loss: 1.3746 G_loss: 0.7178 D_real_acc: 0.5625 D_fake_acc: 0.5781\n",
      "Epoch [11/50] Batch [800/938] D_loss: 1.3234 G_loss: 0.7774 D_real_acc: 0.5469 D_fake_acc: 0.6562\n",
      "Epoch [11/50] Batch [900/938] D_loss: 1.3761 G_loss: 0.6582 D_real_acc: 0.6562 D_fake_acc: 0.3594\n",
      "Epoch [11/50] Average - D_loss: 1.3658 G_loss: 0.7439 D_real_acc: 0.4566 D_fake_acc: 0.6509\n",
      "Epoch [12/50] Batch [0/938] D_loss: 1.3779 G_loss: 0.7368 D_real_acc: 0.3906 D_fake_acc: 0.6719\n",
      "Epoch [12/50] Batch [100/938] D_loss: 1.3291 G_loss: 0.7580 D_real_acc: 0.4531 D_fake_acc: 0.8125\n",
      "Epoch [12/50] Batch [200/938] D_loss: 1.3796 G_loss: 0.7303 D_real_acc: 0.4531 D_fake_acc: 0.6094\n",
      "Epoch [12/50] Batch [300/938] D_loss: 1.3510 G_loss: 0.7274 D_real_acc: 0.4062 D_fake_acc: 0.7344\n",
      "Epoch [12/50] Batch [400/938] D_loss: 1.3589 G_loss: 0.7399 D_real_acc: 0.3438 D_fake_acc: 0.7188\n",
      "Epoch [12/50] Batch [500/938] D_loss: 1.3731 G_loss: 0.7508 D_real_acc: 0.3438 D_fake_acc: 0.7812\n",
      "Epoch [12/50] Batch [600/938] D_loss: 1.3530 G_loss: 0.7187 D_real_acc: 0.5781 D_fake_acc: 0.5938\n",
      "Epoch [12/50] Batch [700/938] D_loss: 1.3706 G_loss: 0.7712 D_real_acc: 0.2656 D_fake_acc: 0.7969\n",
      "Epoch [12/50] Batch [800/938] D_loss: 1.3443 G_loss: 0.6911 D_real_acc: 0.7031 D_fake_acc: 0.4219\n",
      "Epoch [12/50] Batch [900/938] D_loss: 1.3736 G_loss: 0.7331 D_real_acc: 0.5625 D_fake_acc: 0.6719\n",
      "Epoch [12/50] Average - D_loss: 1.3676 G_loss: 0.7373 D_real_acc: 0.4614 D_fake_acc: 0.6454\n",
      "Epoch [13/50] Batch [0/938] D_loss: 1.3820 G_loss: 0.6966 D_real_acc: 0.5469 D_fake_acc: 0.5000\n",
      "Epoch [13/50] Batch [100/938] D_loss: 1.3846 G_loss: 0.7392 D_real_acc: 0.4844 D_fake_acc: 0.5781\n",
      "Epoch [13/50] Batch [200/938] D_loss: 1.3868 G_loss: 0.7219 D_real_acc: 0.5625 D_fake_acc: 0.5938\n",
      "Epoch [13/50] Batch [300/938] D_loss: 1.3449 G_loss: 0.7517 D_real_acc: 0.4375 D_fake_acc: 0.7500\n",
      "Epoch [13/50] Batch [400/938] D_loss: 1.3556 G_loss: 0.7181 D_real_acc: 0.5469 D_fake_acc: 0.6875\n",
      "Epoch [13/50] Batch [500/938] D_loss: 1.3761 G_loss: 0.7608 D_real_acc: 0.5000 D_fake_acc: 0.5625\n",
      "Epoch [13/50] Batch [600/938] D_loss: 1.4156 G_loss: 0.7628 D_real_acc: 0.3125 D_fake_acc: 0.6562\n",
      "Epoch [13/50] Batch [700/938] D_loss: 1.3805 G_loss: 0.7075 D_real_acc: 0.5312 D_fake_acc: 0.4219\n",
      "Epoch [13/50] Batch [800/938] D_loss: 1.3163 G_loss: 0.7038 D_real_acc: 0.4375 D_fake_acc: 0.7188\n",
      "Epoch [13/50] Batch [900/938] D_loss: 1.3831 G_loss: 0.7143 D_real_acc: 0.2812 D_fake_acc: 0.7812\n",
      "Epoch [13/50] Average - D_loss: 1.3680 G_loss: 0.7378 D_real_acc: 0.4672 D_fake_acc: 0.6387\n",
      "Epoch [14/50] Batch [0/938] D_loss: 1.3938 G_loss: 0.7235 D_real_acc: 0.3438 D_fake_acc: 0.6719\n",
      "Epoch [14/50] Batch [100/938] D_loss: 1.3556 G_loss: 0.7112 D_real_acc: 0.6250 D_fake_acc: 0.5312\n",
      "Epoch [14/50] Batch [200/938] D_loss: 1.3900 G_loss: 0.7026 D_real_acc: 0.3906 D_fake_acc: 0.6562\n",
      "Epoch [14/50] Batch [300/938] D_loss: 1.3686 G_loss: 0.7122 D_real_acc: 0.4844 D_fake_acc: 0.5938\n",
      "Epoch [14/50] Batch [400/938] D_loss: 1.3533 G_loss: 0.7356 D_real_acc: 0.3906 D_fake_acc: 0.8125\n",
      "Epoch [14/50] Batch [500/938] D_loss: 1.3725 G_loss: 0.7334 D_real_acc: 0.3750 D_fake_acc: 0.7031\n",
      "Epoch [14/50] Batch [600/938] D_loss: 1.3597 G_loss: 0.7062 D_real_acc: 0.5000 D_fake_acc: 0.6094\n",
      "Epoch [14/50] Batch [700/938] D_loss: 1.3961 G_loss: 0.6969 D_real_acc: 0.4375 D_fake_acc: 0.5000\n",
      "Epoch [14/50] Batch [800/938] D_loss: 1.3746 G_loss: 0.7030 D_real_acc: 0.4375 D_fake_acc: 0.5312\n",
      "Epoch [14/50] Batch [900/938] D_loss: 1.3326 G_loss: 0.7460 D_real_acc: 0.5625 D_fake_acc: 0.6094\n",
      "Epoch [14/50] Average - D_loss: 1.3712 G_loss: 0.7298 D_real_acc: 0.4672 D_fake_acc: 0.6255\n",
      "Epoch [15/50] Batch [0/938] D_loss: 1.3952 G_loss: 0.7167 D_real_acc: 0.4531 D_fake_acc: 0.5938\n",
      "Epoch [15/50] Batch [100/938] D_loss: 1.3820 G_loss: 0.7492 D_real_acc: 0.3281 D_fake_acc: 0.6875\n",
      "Epoch [15/50] Batch [200/938] D_loss: 1.3670 G_loss: 0.8138 D_real_acc: 0.1562 D_fake_acc: 0.9375\n",
      "Epoch [15/50] Batch [300/938] D_loss: 1.3439 G_loss: 0.7179 D_real_acc: 0.7031 D_fake_acc: 0.4375\n",
      "Epoch [15/50] Batch [400/938] D_loss: 1.3673 G_loss: 0.7261 D_real_acc: 0.5156 D_fake_acc: 0.6094\n",
      "Epoch [15/50] Batch [500/938] D_loss: 1.3914 G_loss: 0.7693 D_real_acc: 0.3125 D_fake_acc: 0.7656\n",
      "Epoch [15/50] Batch [600/938] D_loss: 1.3692 G_loss: 0.7741 D_real_acc: 0.4844 D_fake_acc: 0.5312\n",
      "Epoch [15/50] Batch [700/938] D_loss: 1.3506 G_loss: 0.6857 D_real_acc: 0.6406 D_fake_acc: 0.5625\n",
      "Epoch [15/50] Batch [800/938] D_loss: 1.3640 G_loss: 0.7238 D_real_acc: 0.4531 D_fake_acc: 0.6875\n",
      "Epoch [15/50] Batch [900/938] D_loss: 1.3144 G_loss: 0.7137 D_real_acc: 0.5938 D_fake_acc: 0.6562\n",
      "Epoch [15/50] Average - D_loss: 1.3710 G_loss: 0.7292 D_real_acc: 0.4688 D_fake_acc: 0.6271\n",
      "Generated images saved to generated_images/epoch_15.png\n",
      "Epoch [16/50] Batch [0/938] D_loss: 1.3787 G_loss: 0.7296 D_real_acc: 0.4844 D_fake_acc: 0.5156\n",
      "Epoch [16/50] Batch [100/938] D_loss: 1.3693 G_loss: 0.7291 D_real_acc: 0.3125 D_fake_acc: 0.7031\n",
      "Epoch [16/50] Batch [200/938] D_loss: 1.3911 G_loss: 0.7129 D_real_acc: 0.5312 D_fake_acc: 0.6094\n",
      "Epoch [16/50] Batch [300/938] D_loss: 1.3755 G_loss: 0.7592 D_real_acc: 0.2344 D_fake_acc: 0.8281\n",
      "Epoch [16/50] Batch [400/938] D_loss: 1.3537 G_loss: 0.7196 D_real_acc: 0.5938 D_fake_acc: 0.4844\n",
      "Epoch [16/50] Batch [500/938] D_loss: 1.3768 G_loss: 0.7573 D_real_acc: 0.2812 D_fake_acc: 0.7656\n",
      "Epoch [16/50] Batch [600/938] D_loss: 1.3642 G_loss: 0.7236 D_real_acc: 0.5469 D_fake_acc: 0.5000\n",
      "Epoch [16/50] Batch [700/938] D_loss: 1.3581 G_loss: 0.7639 D_real_acc: 0.4375 D_fake_acc: 0.6875\n",
      "Epoch [16/50] Batch [800/938] D_loss: 1.3800 G_loss: 0.7040 D_real_acc: 0.4844 D_fake_acc: 0.6250\n",
      "Epoch [16/50] Batch [900/938] D_loss: 1.3935 G_loss: 0.7473 D_real_acc: 0.3125 D_fake_acc: 0.7031\n",
      "Epoch [16/50] Average - D_loss: 1.3741 G_loss: 0.7251 D_real_acc: 0.4635 D_fake_acc: 0.6241\n",
      "Epoch [17/50] Batch [0/938] D_loss: 1.3732 G_loss: 0.7290 D_real_acc: 0.2656 D_fake_acc: 0.7812\n",
      "Epoch [17/50] Batch [100/938] D_loss: 1.3939 G_loss: 0.7231 D_real_acc: 0.3750 D_fake_acc: 0.6250\n",
      "Epoch [17/50] Batch [200/938] D_loss: 1.3575 G_loss: 0.7336 D_real_acc: 0.2969 D_fake_acc: 0.7344\n",
      "Epoch [17/50] Batch [300/938] D_loss: 1.4089 G_loss: 0.7080 D_real_acc: 0.2656 D_fake_acc: 0.6562\n",
      "Epoch [17/50] Batch [400/938] D_loss: 1.3625 G_loss: 0.6765 D_real_acc: 0.5781 D_fake_acc: 0.5469\n",
      "Epoch [17/50] Batch [500/938] D_loss: 1.3680 G_loss: 0.7281 D_real_acc: 0.4219 D_fake_acc: 0.5781\n",
      "Epoch [17/50] Batch [600/938] D_loss: 1.3791 G_loss: 0.7106 D_real_acc: 0.4219 D_fake_acc: 0.4844\n",
      "Epoch [17/50] Batch [700/938] D_loss: 1.3850 G_loss: 0.7049 D_real_acc: 0.5469 D_fake_acc: 0.5625\n",
      "Epoch [17/50] Batch [800/938] D_loss: 1.3836 G_loss: 0.7187 D_real_acc: 0.4062 D_fake_acc: 0.6406\n",
      "Epoch [17/50] Batch [900/938] D_loss: 1.3745 G_loss: 0.7094 D_real_acc: 0.3281 D_fake_acc: 0.7656\n",
      "Epoch [17/50] Average - D_loss: 1.3771 G_loss: 0.7196 D_real_acc: 0.4532 D_fake_acc: 0.6188\n",
      "Epoch [18/50] Batch [0/938] D_loss: 1.3964 G_loss: 0.7171 D_real_acc: 0.4375 D_fake_acc: 0.4844\n",
      "Epoch [18/50] Batch [100/938] D_loss: 1.3933 G_loss: 0.7149 D_real_acc: 0.5156 D_fake_acc: 0.4688\n",
      "Epoch [18/50] Batch [200/938] D_loss: 1.4016 G_loss: 0.7331 D_real_acc: 0.4062 D_fake_acc: 0.6094\n",
      "Epoch [18/50] Batch [300/938] D_loss: 1.3827 G_loss: 0.7197 D_real_acc: 0.5156 D_fake_acc: 0.5938\n",
      "Epoch [18/50] Batch [400/938] D_loss: 1.3662 G_loss: 0.7155 D_real_acc: 0.5781 D_fake_acc: 0.4375\n",
      "Epoch [18/50] Batch [500/938] D_loss: 1.3746 G_loss: 0.7172 D_real_acc: 0.6250 D_fake_acc: 0.5469\n",
      "Epoch [18/50] Batch [600/938] D_loss: 1.4006 G_loss: 0.7407 D_real_acc: 0.4062 D_fake_acc: 0.5625\n",
      "Epoch [18/50] Batch [700/938] D_loss: 1.3660 G_loss: 0.7144 D_real_acc: 0.5000 D_fake_acc: 0.5312\n",
      "Epoch [18/50] Batch [800/938] D_loss: 1.3717 G_loss: 0.7407 D_real_acc: 0.3125 D_fake_acc: 0.7500\n",
      "Epoch [18/50] Batch [900/938] D_loss: 1.3655 G_loss: 0.6825 D_real_acc: 0.6562 D_fake_acc: 0.5469\n",
      "Epoch [18/50] Average - D_loss: 1.3781 G_loss: 0.7166 D_real_acc: 0.4608 D_fake_acc: 0.6116\n",
      "Epoch [19/50] Batch [0/938] D_loss: 1.3725 G_loss: 0.6734 D_real_acc: 0.4688 D_fake_acc: 0.6562\n",
      "Epoch [19/50] Batch [100/938] D_loss: 1.3776 G_loss: 0.7122 D_real_acc: 0.4062 D_fake_acc: 0.6094\n",
      "Epoch [19/50] Batch [200/938] D_loss: 1.3836 G_loss: 0.7009 D_real_acc: 0.6719 D_fake_acc: 0.3438\n",
      "Epoch [19/50] Batch [300/938] D_loss: 1.3844 G_loss: 0.7146 D_real_acc: 0.5156 D_fake_acc: 0.5781\n",
      "Epoch [19/50] Batch [400/938] D_loss: 1.3804 G_loss: 0.7275 D_real_acc: 0.3906 D_fake_acc: 0.5781\n",
      "Epoch [19/50] Batch [500/938] D_loss: 1.3817 G_loss: 0.7169 D_real_acc: 0.5469 D_fake_acc: 0.5156\n",
      "Epoch [19/50] Batch [600/938] D_loss: 1.3618 G_loss: 0.6906 D_real_acc: 0.6406 D_fake_acc: 0.5312\n",
      "Epoch [19/50] Batch [700/938] D_loss: 1.3895 G_loss: 0.7152 D_real_acc: 0.4219 D_fake_acc: 0.5938\n",
      "Epoch [19/50] Batch [800/938] D_loss: 1.4073 G_loss: 0.6978 D_real_acc: 0.4062 D_fake_acc: 0.4844\n",
      "Epoch [19/50] Batch [900/938] D_loss: 1.3746 G_loss: 0.7031 D_real_acc: 0.4531 D_fake_acc: 0.5938\n",
      "Epoch [19/50] Average - D_loss: 1.3787 G_loss: 0.7127 D_real_acc: 0.4725 D_fake_acc: 0.5961\n",
      "Epoch [20/50] Batch [0/938] D_loss: 1.3987 G_loss: 0.7404 D_real_acc: 0.4219 D_fake_acc: 0.5938\n",
      "Epoch [20/50] Batch [100/938] D_loss: 1.3719 G_loss: 0.7063 D_real_acc: 0.4062 D_fake_acc: 0.6719\n",
      "Epoch [20/50] Batch [200/938] D_loss: 1.3863 G_loss: 0.7383 D_real_acc: 0.3438 D_fake_acc: 0.7344\n",
      "Epoch [20/50] Batch [300/938] D_loss: 1.3950 G_loss: 0.7371 D_real_acc: 0.2969 D_fake_acc: 0.6719\n",
      "Epoch [20/50] Batch [400/938] D_loss: 1.3840 G_loss: 0.7192 D_real_acc: 0.4531 D_fake_acc: 0.6250\n",
      "Epoch [20/50] Batch [500/938] D_loss: 1.3781 G_loss: 0.6926 D_real_acc: 0.5781 D_fake_acc: 0.5000\n",
      "Epoch [20/50] Batch [600/938] D_loss: 1.3849 G_loss: 0.7231 D_real_acc: 0.4062 D_fake_acc: 0.4688\n",
      "Epoch [20/50] Batch [700/938] D_loss: 1.3928 G_loss: 0.7279 D_real_acc: 0.2969 D_fake_acc: 0.6250\n",
      "Epoch [20/50] Batch [800/938] D_loss: 1.3942 G_loss: 0.7380 D_real_acc: 0.2031 D_fake_acc: 0.7188\n",
      "Epoch [20/50] Batch [900/938] D_loss: 1.3712 G_loss: 0.7031 D_real_acc: 0.6250 D_fake_acc: 0.5938\n",
      "Epoch [20/50] Average - D_loss: 1.3794 G_loss: 0.7144 D_real_acc: 0.4577 D_fake_acc: 0.6043\n",
      "Generated images saved to generated_images/epoch_20.png\n",
      "Epoch [21/50] Batch [0/938] D_loss: 1.3782 G_loss: 0.7078 D_real_acc: 0.5156 D_fake_acc: 0.5312\n",
      "Epoch [21/50] Batch [100/938] D_loss: 1.3846 G_loss: 0.6970 D_real_acc: 0.5781 D_fake_acc: 0.6562\n",
      "Epoch [21/50] Batch [200/938] D_loss: 1.3782 G_loss: 0.7015 D_real_acc: 0.5000 D_fake_acc: 0.4844\n",
      "Epoch [21/50] Batch [300/938] D_loss: 1.3934 G_loss: 0.7550 D_real_acc: 0.2031 D_fake_acc: 0.7812\n",
      "Epoch [21/50] Batch [400/938] D_loss: 1.3944 G_loss: 0.7416 D_real_acc: 0.3281 D_fake_acc: 0.6250\n",
      "Epoch [21/50] Batch [500/938] D_loss: 1.3653 G_loss: 0.6960 D_real_acc: 0.5312 D_fake_acc: 0.5938\n",
      "Epoch [21/50] Batch [600/938] D_loss: 1.4118 G_loss: 0.7373 D_real_acc: 0.4375 D_fake_acc: 0.4375\n",
      "Epoch [21/50] Batch [700/938] D_loss: 1.3871 G_loss: 0.7359 D_real_acc: 0.4531 D_fake_acc: 0.5625\n",
      "Epoch [21/50] Batch [800/938] D_loss: 1.3941 G_loss: 0.7299 D_real_acc: 0.2188 D_fake_acc: 0.7031\n",
      "Epoch [21/50] Batch [900/938] D_loss: 1.3867 G_loss: 0.7087 D_real_acc: 0.4844 D_fake_acc: 0.5469\n",
      "Epoch [21/50] Average - D_loss: 1.3802 G_loss: 0.7106 D_real_acc: 0.4562 D_fake_acc: 0.6003\n",
      "Epoch [22/50] Batch [0/938] D_loss: 1.3796 G_loss: 0.7282 D_real_acc: 0.4688 D_fake_acc: 0.5781\n",
      "Epoch [22/50] Batch [100/938] D_loss: 1.3633 G_loss: 0.6976 D_real_acc: 0.3906 D_fake_acc: 0.7344\n",
      "Epoch [22/50] Batch [200/938] D_loss: 1.3652 G_loss: 0.6838 D_real_acc: 0.6562 D_fake_acc: 0.5156\n",
      "Epoch [22/50] Batch [300/938] D_loss: 1.3701 G_loss: 0.7131 D_real_acc: 0.4219 D_fake_acc: 0.6875\n",
      "Epoch [22/50] Batch [400/938] D_loss: 1.3803 G_loss: 0.7253 D_real_acc: 0.2656 D_fake_acc: 0.7969\n",
      "Epoch [22/50] Batch [500/938] D_loss: 1.3716 G_loss: 0.7146 D_real_acc: 0.4219 D_fake_acc: 0.6875\n",
      "Epoch [22/50] Batch [600/938] D_loss: 1.3990 G_loss: 0.7233 D_real_acc: 0.3594 D_fake_acc: 0.5781\n",
      "Epoch [22/50] Batch [700/938] D_loss: 1.3755 G_loss: 0.7230 D_real_acc: 0.4219 D_fake_acc: 0.6719\n",
      "Epoch [22/50] Batch [800/938] D_loss: 1.3766 G_loss: 0.7213 D_real_acc: 0.3281 D_fake_acc: 0.6406\n",
      "Epoch [22/50] Batch [900/938] D_loss: 1.3675 G_loss: 0.7056 D_real_acc: 0.5469 D_fake_acc: 0.6719\n",
      "Epoch [22/50] Average - D_loss: 1.3809 G_loss: 0.7109 D_real_acc: 0.4465 D_fake_acc: 0.6078\n",
      "Epoch [23/50] Batch [0/938] D_loss: 1.3830 G_loss: 0.7578 D_real_acc: 0.2656 D_fake_acc: 0.7969\n",
      "Epoch [23/50] Batch [100/938] D_loss: 1.3768 G_loss: 0.6867 D_real_acc: 0.6094 D_fake_acc: 0.5000\n",
      "Epoch [23/50] Batch [200/938] D_loss: 1.3820 G_loss: 0.6948 D_real_acc: 0.4531 D_fake_acc: 0.6719\n",
      "Epoch [23/50] Batch [300/938] D_loss: 1.3956 G_loss: 0.7072 D_real_acc: 0.3906 D_fake_acc: 0.6562\n",
      "Epoch [23/50] Batch [400/938] D_loss: 1.3828 G_loss: 0.7179 D_real_acc: 0.4844 D_fake_acc: 0.5156\n",
      "Epoch [23/50] Batch [500/938] D_loss: 1.3905 G_loss: 0.7274 D_real_acc: 0.2656 D_fake_acc: 0.6719\n",
      "Epoch [23/50] Batch [600/938] D_loss: 1.3915 G_loss: 0.6805 D_real_acc: 0.5625 D_fake_acc: 0.5000\n",
      "Epoch [23/50] Batch [700/938] D_loss: 1.3757 G_loss: 0.6761 D_real_acc: 0.7969 D_fake_acc: 0.4219\n",
      "Epoch [23/50] Batch [800/938] D_loss: 1.3812 G_loss: 0.6971 D_real_acc: 0.5781 D_fake_acc: 0.4375\n",
      "Epoch [23/50] Batch [900/938] D_loss: 1.3763 G_loss: 0.7079 D_real_acc: 0.6094 D_fake_acc: 0.4844\n",
      "Epoch [23/50] Average - D_loss: 1.3807 G_loss: 0.7090 D_real_acc: 0.4633 D_fake_acc: 0.5930\n",
      "Epoch [24/50] Batch [0/938] D_loss: 1.3826 G_loss: 0.7014 D_real_acc: 0.4844 D_fake_acc: 0.5469\n",
      "Epoch [24/50] Batch [100/938] D_loss: 1.3618 G_loss: 0.7041 D_real_acc: 0.5625 D_fake_acc: 0.5625\n",
      "Epoch [24/50] Batch [200/938] D_loss: 1.3839 G_loss: 0.7053 D_real_acc: 0.5625 D_fake_acc: 0.4688\n",
      "Epoch [24/50] Batch [300/938] D_loss: 1.3964 G_loss: 0.7089 D_real_acc: 0.2969 D_fake_acc: 0.5938\n",
      "Epoch [24/50] Batch [400/938] D_loss: 1.3752 G_loss: 0.6948 D_real_acc: 0.5938 D_fake_acc: 0.6094\n",
      "Epoch [24/50] Batch [500/938] D_loss: 1.3679 G_loss: 0.7120 D_real_acc: 0.3750 D_fake_acc: 0.7031\n",
      "Epoch [24/50] Batch [600/938] D_loss: 1.3750 G_loss: 0.6993 D_real_acc: 0.4219 D_fake_acc: 0.6406\n",
      "Epoch [24/50] Batch [700/938] D_loss: 1.3983 G_loss: 0.6891 D_real_acc: 0.5469 D_fake_acc: 0.4375\n",
      "Epoch [24/50] Batch [800/938] D_loss: 1.3900 G_loss: 0.7024 D_real_acc: 0.4688 D_fake_acc: 0.5000\n",
      "Epoch [24/50] Batch [900/938] D_loss: 1.4074 G_loss: 0.7023 D_real_acc: 0.6406 D_fake_acc: 0.3125\n",
      "Epoch [24/50] Average - D_loss: 1.3825 G_loss: 0.7054 D_real_acc: 0.4515 D_fake_acc: 0.5934\n",
      "Epoch [25/50] Batch [0/938] D_loss: 1.3817 G_loss: 0.7116 D_real_acc: 0.5781 D_fake_acc: 0.4688\n",
      "Epoch [25/50] Batch [100/938] D_loss: 1.3776 G_loss: 0.6695 D_real_acc: 0.6719 D_fake_acc: 0.3281\n",
      "Epoch [25/50] Batch [200/938] D_loss: 1.3845 G_loss: 0.7237 D_real_acc: 0.4062 D_fake_acc: 0.7188\n",
      "Epoch [25/50] Batch [300/938] D_loss: 1.3837 G_loss: 0.6847 D_real_acc: 0.5000 D_fake_acc: 0.4531\n",
      "Epoch [25/50] Batch [400/938] D_loss: 1.3876 G_loss: 0.7278 D_real_acc: 0.3281 D_fake_acc: 0.7188\n",
      "Epoch [25/50] Batch [500/938] D_loss: 1.3709 G_loss: 0.7274 D_real_acc: 0.4688 D_fake_acc: 0.5938\n",
      "Epoch [25/50] Batch [600/938] D_loss: 1.3977 G_loss: 0.6845 D_real_acc: 0.5938 D_fake_acc: 0.3594\n",
      "Epoch [25/50] Batch [700/938] D_loss: 1.3802 G_loss: 0.7125 D_real_acc: 0.6406 D_fake_acc: 0.4531\n",
      "Epoch [25/50] Batch [800/938] D_loss: 1.3890 G_loss: 0.7022 D_real_acc: 0.6562 D_fake_acc: 0.3750\n",
      "Epoch [25/50] Batch [900/938] D_loss: 1.3872 G_loss: 0.7067 D_real_acc: 0.3438 D_fake_acc: 0.5938\n",
      "Epoch [25/50] Average - D_loss: 1.3829 G_loss: 0.7050 D_real_acc: 0.4421 D_fake_acc: 0.6011\n",
      "Generated images saved to generated_images/epoch_25.png\n",
      "Epoch [26/50] Batch [0/938] D_loss: 1.3925 G_loss: 0.7111 D_real_acc: 0.3594 D_fake_acc: 0.7031\n",
      "Epoch [26/50] Batch [100/938] D_loss: 1.3819 G_loss: 0.7070 D_real_acc: 0.3750 D_fake_acc: 0.7031\n",
      "Epoch [26/50] Batch [200/938] D_loss: 1.3413 G_loss: 0.6458 D_real_acc: 0.8594 D_fake_acc: 0.3906\n",
      "Epoch [26/50] Batch [300/938] D_loss: 1.3733 G_loss: 0.6810 D_real_acc: 0.5625 D_fake_acc: 0.4531\n",
      "Epoch [26/50] Batch [400/938] D_loss: 1.3822 G_loss: 0.6979 D_real_acc: 0.6250 D_fake_acc: 0.4375\n",
      "Epoch [26/50] Batch [500/938] D_loss: 1.3931 G_loss: 0.7056 D_real_acc: 0.4375 D_fake_acc: 0.4688\n",
      "Epoch [26/50] Batch [600/938] D_loss: 1.3907 G_loss: 0.7158 D_real_acc: 0.2188 D_fake_acc: 0.6719\n",
      "Epoch [26/50] Batch [700/938] D_loss: 1.3845 G_loss: 0.7152 D_real_acc: 0.4219 D_fake_acc: 0.6250\n",
      "Epoch [26/50] Batch [800/938] D_loss: 1.3975 G_loss: 0.7171 D_real_acc: 0.3750 D_fake_acc: 0.5312\n",
      "Epoch [26/50] Batch [900/938] D_loss: 1.3915 G_loss: 0.6987 D_real_acc: 0.4844 D_fake_acc: 0.5469\n",
      "Epoch [26/50] Average - D_loss: 1.3827 G_loss: 0.7049 D_real_acc: 0.4496 D_fake_acc: 0.5954\n",
      "Epoch [27/50] Batch [0/938] D_loss: 1.4080 G_loss: 0.7097 D_real_acc: 0.3906 D_fake_acc: 0.4844\n",
      "Epoch [27/50] Batch [100/938] D_loss: 1.3719 G_loss: 0.6911 D_real_acc: 0.4062 D_fake_acc: 0.6875\n",
      "Epoch [27/50] Batch [200/938] D_loss: 1.3895 G_loss: 0.7277 D_real_acc: 0.1719 D_fake_acc: 0.7969\n",
      "Epoch [27/50] Batch [300/938] D_loss: 1.3785 G_loss: 0.6865 D_real_acc: 0.5469 D_fake_acc: 0.4844\n",
      "Epoch [27/50] Batch [400/938] D_loss: 1.3941 G_loss: 0.7037 D_real_acc: 0.3281 D_fake_acc: 0.5938\n",
      "Epoch [27/50] Batch [500/938] D_loss: 1.3776 G_loss: 0.6861 D_real_acc: 0.6406 D_fake_acc: 0.5625\n",
      "Epoch [27/50] Batch [600/938] D_loss: 1.3858 G_loss: 0.7200 D_real_acc: 0.3438 D_fake_acc: 0.7344\n",
      "Epoch [27/50] Batch [700/938] D_loss: 1.3958 G_loss: 0.7088 D_real_acc: 0.4531 D_fake_acc: 0.5781\n",
      "Epoch [27/50] Batch [800/938] D_loss: 1.4042 G_loss: 0.7240 D_real_acc: 0.1875 D_fake_acc: 0.7812\n",
      "Epoch [27/50] Batch [900/938] D_loss: 1.3828 G_loss: 0.6911 D_real_acc: 0.5469 D_fake_acc: 0.3750\n",
      "Epoch [27/50] Average - D_loss: 1.3832 G_loss: 0.7048 D_real_acc: 0.4501 D_fake_acc: 0.5904\n",
      "Epoch [28/50] Batch [0/938] D_loss: 1.3832 G_loss: 0.7059 D_real_acc: 0.5781 D_fake_acc: 0.5469\n",
      "Epoch [28/50] Batch [100/938] D_loss: 1.3761 G_loss: 0.6992 D_real_acc: 0.5312 D_fake_acc: 0.5625\n",
      "Epoch [28/50] Batch [200/938] D_loss: 1.3826 G_loss: 0.6888 D_real_acc: 0.5469 D_fake_acc: 0.4688\n",
      "Epoch [28/50] Batch [300/938] D_loss: 1.3846 G_loss: 0.7184 D_real_acc: 0.4062 D_fake_acc: 0.5625\n",
      "Epoch [28/50] Batch [400/938] D_loss: 1.3914 G_loss: 0.7045 D_real_acc: 0.3125 D_fake_acc: 0.6406\n",
      "Epoch [28/50] Batch [500/938] D_loss: 1.3747 G_loss: 0.7339 D_real_acc: 0.5625 D_fake_acc: 0.5938\n",
      "Epoch [28/50] Batch [600/938] D_loss: 1.3849 G_loss: 0.7041 D_real_acc: 0.3594 D_fake_acc: 0.6250\n",
      "Epoch [28/50] Batch [700/938] D_loss: 1.3937 G_loss: 0.7528 D_real_acc: 0.3750 D_fake_acc: 0.6562\n",
      "Epoch [28/50] Batch [800/938] D_loss: 1.3657 G_loss: 0.6893 D_real_acc: 0.6875 D_fake_acc: 0.4531\n",
      "Epoch [28/50] Batch [900/938] D_loss: 1.3659 G_loss: 0.7085 D_real_acc: 0.6562 D_fake_acc: 0.5469\n",
      "Epoch [28/50] Average - D_loss: 1.3842 G_loss: 0.7022 D_real_acc: 0.4522 D_fake_acc: 0.5838\n",
      "Epoch [29/50] Batch [0/938] D_loss: 1.3905 G_loss: 0.7052 D_real_acc: 0.4844 D_fake_acc: 0.6094\n",
      "Epoch [29/50] Batch [100/938] D_loss: 1.3743 G_loss: 0.6482 D_real_acc: 0.8281 D_fake_acc: 0.2188\n",
      "Epoch [29/50] Batch [200/938] D_loss: 1.3753 G_loss: 0.6863 D_real_acc: 0.5625 D_fake_acc: 0.5156\n",
      "Epoch [29/50] Batch [300/938] D_loss: 1.3762 G_loss: 0.7146 D_real_acc: 0.2812 D_fake_acc: 0.8125\n",
      "Epoch [29/50] Batch [400/938] D_loss: 1.4008 G_loss: 0.6852 D_real_acc: 0.6094 D_fake_acc: 0.3750\n",
      "Epoch [29/50] Batch [500/938] D_loss: 1.3758 G_loss: 0.7149 D_real_acc: 0.3438 D_fake_acc: 0.7344\n",
      "Epoch [29/50] Batch [600/938] D_loss: 1.4109 G_loss: 0.6903 D_real_acc: 0.5938 D_fake_acc: 0.2344\n",
      "Epoch [29/50] Batch [700/938] D_loss: 1.3940 G_loss: 0.7135 D_real_acc: 0.3125 D_fake_acc: 0.7031\n",
      "Epoch [29/50] Batch [800/938] D_loss: 1.3760 G_loss: 0.7081 D_real_acc: 0.4688 D_fake_acc: 0.7031\n",
      "Epoch [29/50] Batch [900/938] D_loss: 1.3945 G_loss: 0.7301 D_real_acc: 0.0938 D_fake_acc: 0.8438\n",
      "Epoch [29/50] Average - D_loss: 1.3843 G_loss: 0.7007 D_real_acc: 0.4462 D_fake_acc: 0.5844\n",
      "Epoch [30/50] Batch [0/938] D_loss: 1.3867 G_loss: 0.6628 D_real_acc: 0.8281 D_fake_acc: 0.1875\n",
      "Epoch [30/50] Batch [100/938] D_loss: 1.3864 G_loss: 0.7331 D_real_acc: 0.4844 D_fake_acc: 0.5469\n",
      "Epoch [30/50] Batch [200/938] D_loss: 1.3892 G_loss: 0.7092 D_real_acc: 0.3125 D_fake_acc: 0.7969\n",
      "Epoch [30/50] Batch [300/938] D_loss: 1.3886 G_loss: 0.6945 D_real_acc: 0.4688 D_fake_acc: 0.5469\n",
      "Epoch [30/50] Batch [400/938] D_loss: 1.3706 G_loss: 0.6992 D_real_acc: 0.5000 D_fake_acc: 0.5000\n",
      "Epoch [30/50] Batch [500/938] D_loss: 1.3899 G_loss: 0.7282 D_real_acc: 0.1094 D_fake_acc: 0.8750\n",
      "Epoch [30/50] Batch [600/938] D_loss: 1.3755 G_loss: 0.7060 D_real_acc: 0.4375 D_fake_acc: 0.6719\n",
      "Epoch [30/50] Batch [700/938] D_loss: 1.3962 G_loss: 0.7067 D_real_acc: 0.3438 D_fake_acc: 0.5781\n",
      "Epoch [30/50] Batch [800/938] D_loss: 1.3861 G_loss: 0.7202 D_real_acc: 0.2656 D_fake_acc: 0.7031\n",
      "Epoch [30/50] Batch [900/938] D_loss: 1.3860 G_loss: 0.6893 D_real_acc: 0.6094 D_fake_acc: 0.3750\n",
      "Epoch [30/50] Average - D_loss: 1.3846 G_loss: 0.7008 D_real_acc: 0.4366 D_fake_acc: 0.5936\n",
      "Generated images saved to generated_images/epoch_30.png\n",
      "Epoch [31/50] Batch [0/938] D_loss: 1.3847 G_loss: 0.7065 D_real_acc: 0.3281 D_fake_acc: 0.7969\n",
      "Epoch [31/50] Batch [100/938] D_loss: 1.3881 G_loss: 0.7232 D_real_acc: 0.3281 D_fake_acc: 0.7500\n",
      "Epoch [31/50] Batch [200/938] D_loss: 1.3879 G_loss: 0.6676 D_real_acc: 0.5625 D_fake_acc: 0.3906\n",
      "Epoch [31/50] Batch [300/938] D_loss: 1.3873 G_loss: 0.6971 D_real_acc: 0.4219 D_fake_acc: 0.6250\n",
      "Epoch [31/50] Batch [400/938] D_loss: 1.3819 G_loss: 0.6795 D_real_acc: 0.6094 D_fake_acc: 0.4844\n",
      "Epoch [31/50] Batch [500/938] D_loss: 1.3769 G_loss: 0.6976 D_real_acc: 0.5625 D_fake_acc: 0.5625\n",
      "Epoch [31/50] Batch [600/938] D_loss: 1.3811 G_loss: 0.6797 D_real_acc: 0.5781 D_fake_acc: 0.4688\n",
      "Epoch [31/50] Batch [700/938] D_loss: 1.3846 G_loss: 0.6940 D_real_acc: 0.5000 D_fake_acc: 0.5469\n",
      "Epoch [31/50] Batch [800/938] D_loss: 1.3851 G_loss: 0.6818 D_real_acc: 0.6094 D_fake_acc: 0.4219\n",
      "Epoch [31/50] Batch [900/938] D_loss: 1.3893 G_loss: 0.7186 D_real_acc: 0.1406 D_fake_acc: 0.7969\n",
      "Epoch [31/50] Average - D_loss: 1.3849 G_loss: 0.6992 D_real_acc: 0.4539 D_fake_acc: 0.5718\n",
      "Epoch [32/50] Batch [0/938] D_loss: 1.3824 G_loss: 0.6864 D_real_acc: 0.5312 D_fake_acc: 0.4844\n",
      "Epoch [32/50] Batch [100/938] D_loss: 1.3881 G_loss: 0.7134 D_real_acc: 0.1250 D_fake_acc: 0.8281\n",
      "Epoch [32/50] Batch [200/938] D_loss: 1.3779 G_loss: 0.7074 D_real_acc: 0.4531 D_fake_acc: 0.6562\n",
      "Epoch [32/50] Batch [300/938] D_loss: 1.3844 G_loss: 0.6934 D_real_acc: 0.5312 D_fake_acc: 0.5000\n",
      "Epoch [32/50] Batch [400/938] D_loss: 1.3805 G_loss: 0.6956 D_real_acc: 0.3438 D_fake_acc: 0.7031\n",
      "Epoch [32/50] Batch [500/938] D_loss: 1.3876 G_loss: 0.6909 D_real_acc: 0.5938 D_fake_acc: 0.4062\n",
      "Epoch [32/50] Batch [600/938] D_loss: 1.3833 G_loss: 0.7004 D_real_acc: 0.4844 D_fake_acc: 0.6094\n",
      "Epoch [32/50] Batch [700/938] D_loss: 1.3888 G_loss: 0.6926 D_real_acc: 0.4844 D_fake_acc: 0.5469\n",
      "Epoch [32/50] Batch [800/938] D_loss: 1.3991 G_loss: 0.7031 D_real_acc: 0.3281 D_fake_acc: 0.5938\n",
      "Epoch [32/50] Batch [900/938] D_loss: 1.3871 G_loss: 0.7062 D_real_acc: 0.2812 D_fake_acc: 0.7812\n",
      "Epoch [32/50] Average - D_loss: 1.3851 G_loss: 0.7000 D_real_acc: 0.4419 D_fake_acc: 0.5841\n",
      "Epoch [33/50] Batch [0/938] D_loss: 1.3903 G_loss: 0.7003 D_real_acc: 0.3438 D_fake_acc: 0.6250\n",
      "Epoch [33/50] Batch [100/938] D_loss: 1.3842 G_loss: 0.7009 D_real_acc: 0.3906 D_fake_acc: 0.6406\n",
      "Epoch [33/50] Batch [200/938] D_loss: 1.3864 G_loss: 0.7146 D_real_acc: 0.2656 D_fake_acc: 0.6875\n",
      "Epoch [33/50] Batch [300/938] D_loss: 1.3862 G_loss: 0.7097 D_real_acc: 0.3281 D_fake_acc: 0.7344\n",
      "Epoch [33/50] Batch [400/938] D_loss: 1.3914 G_loss: 0.6957 D_real_acc: 0.5156 D_fake_acc: 0.4062\n",
      "Epoch [33/50] Batch [500/938] D_loss: 1.3890 G_loss: 0.6753 D_real_acc: 0.7969 D_fake_acc: 0.2344\n",
      "Epoch [33/50] Batch [600/938] D_loss: 1.3848 G_loss: 0.6925 D_real_acc: 0.5781 D_fake_acc: 0.4062\n",
      "Epoch [33/50] Batch [700/938] D_loss: 1.3848 G_loss: 0.7173 D_real_acc: 0.2656 D_fake_acc: 0.7500\n",
      "Epoch [33/50] Batch [800/938] D_loss: 1.3779 G_loss: 0.6863 D_real_acc: 0.6875 D_fake_acc: 0.4062\n",
      "Epoch [33/50] Batch [900/938] D_loss: 1.3833 G_loss: 0.6952 D_real_acc: 0.5781 D_fake_acc: 0.4844\n",
      "Epoch [33/50] Average - D_loss: 1.3853 G_loss: 0.6997 D_real_acc: 0.4373 D_fake_acc: 0.5876\n",
      "Epoch [34/50] Batch [0/938] D_loss: 1.3906 G_loss: 0.7055 D_real_acc: 0.3281 D_fake_acc: 0.6562\n",
      "Epoch [34/50] Batch [100/938] D_loss: 1.3901 G_loss: 0.6756 D_real_acc: 0.7812 D_fake_acc: 0.2344\n",
      "Epoch [34/50] Batch [200/938] D_loss: 1.3870 G_loss: 0.7220 D_real_acc: 0.1719 D_fake_acc: 0.8281\n",
      "Epoch [34/50] Batch [300/938] D_loss: 1.3896 G_loss: 0.6973 D_real_acc: 0.2500 D_fake_acc: 0.6094\n",
      "Epoch [34/50] Batch [400/938] D_loss: 1.3778 G_loss: 0.6818 D_real_acc: 0.5625 D_fake_acc: 0.5625\n",
      "Epoch [34/50] Batch [500/938] D_loss: 1.3934 G_loss: 0.6951 D_real_acc: 0.5000 D_fake_acc: 0.4219\n",
      "Epoch [34/50] Batch [600/938] D_loss: 1.3968 G_loss: 0.6797 D_real_acc: 0.6875 D_fake_acc: 0.4219\n",
      "Epoch [34/50] Batch [700/938] D_loss: 1.3781 G_loss: 0.6690 D_real_acc: 0.6875 D_fake_acc: 0.4375\n",
      "Epoch [34/50] Batch [800/938] D_loss: 1.3830 G_loss: 0.7021 D_real_acc: 0.4531 D_fake_acc: 0.5625\n",
      "Epoch [34/50] Batch [900/938] D_loss: 1.3832 G_loss: 0.6825 D_real_acc: 0.5938 D_fake_acc: 0.4375\n",
      "Epoch [34/50] Average - D_loss: 1.3856 G_loss: 0.6970 D_real_acc: 0.4569 D_fake_acc: 0.5668\n",
      "Epoch [35/50] Batch [0/938] D_loss: 1.3926 G_loss: 0.6928 D_real_acc: 0.5156 D_fake_acc: 0.4688\n",
      "Epoch [35/50] Batch [100/938] D_loss: 1.3878 G_loss: 0.7077 D_real_acc: 0.2812 D_fake_acc: 0.6562\n",
      "Epoch [35/50] Batch [200/938] D_loss: 1.3786 G_loss: 0.7089 D_real_acc: 0.5938 D_fake_acc: 0.5156\n",
      "Epoch [35/50] Batch [300/938] D_loss: 1.3842 G_loss: 0.6753 D_real_acc: 0.5938 D_fake_acc: 0.4531\n",
      "Epoch [35/50] Batch [400/938] D_loss: 1.3802 G_loss: 0.6844 D_real_acc: 0.6250 D_fake_acc: 0.4219\n",
      "Epoch [35/50] Batch [500/938] D_loss: 1.3763 G_loss: 0.6980 D_real_acc: 0.4688 D_fake_acc: 0.6875\n",
      "Epoch [35/50] Batch [600/938] D_loss: 1.3849 G_loss: 0.7068 D_real_acc: 0.4688 D_fake_acc: 0.5938\n",
      "Epoch [35/50] Batch [700/938] D_loss: 1.3687 G_loss: 0.6832 D_real_acc: 0.6875 D_fake_acc: 0.5625\n",
      "Epoch [35/50] Batch [800/938] D_loss: 1.3886 G_loss: 0.6858 D_real_acc: 0.4688 D_fake_acc: 0.4844\n",
      "Epoch [35/50] Batch [900/938] D_loss: 1.3923 G_loss: 0.6902 D_real_acc: 0.5625 D_fake_acc: 0.4062\n",
      "Epoch [35/50] Average - D_loss: 1.3855 G_loss: 0.6980 D_real_acc: 0.4724 D_fake_acc: 0.5524\n",
      "Generated images saved to generated_images/epoch_35.png\n",
      "Epoch [36/50] Batch [0/938] D_loss: 1.3834 G_loss: 0.7121 D_real_acc: 0.3125 D_fake_acc: 0.6875\n",
      "Epoch [36/50] Batch [100/938] D_loss: 1.3857 G_loss: 0.7086 D_real_acc: 0.2812 D_fake_acc: 0.7344\n",
      "Epoch [36/50] Batch [200/938] D_loss: 1.3895 G_loss: 0.7328 D_real_acc: 0.0938 D_fake_acc: 0.8438\n",
      "Epoch [36/50] Batch [300/938] D_loss: 1.3912 G_loss: 0.7114 D_real_acc: 0.2969 D_fake_acc: 0.5938\n",
      "Epoch [36/50] Batch [400/938] D_loss: 1.3823 G_loss: 0.6898 D_real_acc: 0.5781 D_fake_acc: 0.5000\n",
      "Epoch [36/50] Batch [500/938] D_loss: 1.3921 G_loss: 0.7154 D_real_acc: 0.1250 D_fake_acc: 0.7969\n",
      "Epoch [36/50] Batch [600/938] D_loss: 1.3795 G_loss: 0.7034 D_real_acc: 0.5469 D_fake_acc: 0.5625\n",
      "Epoch [36/50] Batch [700/938] D_loss: 1.3878 G_loss: 0.6809 D_real_acc: 0.5938 D_fake_acc: 0.4062\n",
      "Epoch [36/50] Batch [800/938] D_loss: 1.3882 G_loss: 0.6892 D_real_acc: 0.7188 D_fake_acc: 0.2344\n",
      "Epoch [36/50] Batch [900/938] D_loss: 1.3999 G_loss: 0.7012 D_real_acc: 0.2500 D_fake_acc: 0.5625\n",
      "Epoch [36/50] Average - D_loss: 1.3859 G_loss: 0.6977 D_real_acc: 0.4661 D_fake_acc: 0.5520\n",
      "Epoch [37/50] Batch [0/938] D_loss: 1.3854 G_loss: 0.6893 D_real_acc: 0.7344 D_fake_acc: 0.3750\n",
      "Epoch [37/50] Batch [100/938] D_loss: 1.3817 G_loss: 0.7149 D_real_acc: 0.2969 D_fake_acc: 0.7812\n",
      "Epoch [37/50] Batch [200/938] D_loss: 1.3892 G_loss: 0.7203 D_real_acc: 0.1094 D_fake_acc: 0.8750\n",
      "Epoch [37/50] Batch [300/938] D_loss: 1.3907 G_loss: 0.6857 D_real_acc: 0.5469 D_fake_acc: 0.3750\n",
      "Epoch [37/50] Batch [400/938] D_loss: 1.3870 G_loss: 0.7117 D_real_acc: 0.3125 D_fake_acc: 0.6875\n",
      "Epoch [37/50] Batch [500/938] D_loss: 1.3914 G_loss: 0.7002 D_real_acc: 0.2656 D_fake_acc: 0.5938\n",
      "Epoch [37/50] Batch [600/938] D_loss: 1.3877 G_loss: 0.6912 D_real_acc: 0.6562 D_fake_acc: 0.4062\n",
      "Epoch [37/50] Batch [700/938] D_loss: 1.3897 G_loss: 0.7136 D_real_acc: 0.1406 D_fake_acc: 0.8125\n",
      "Epoch [37/50] Batch [800/938] D_loss: 1.3759 G_loss: 0.7013 D_real_acc: 0.4375 D_fake_acc: 0.7812\n",
      "Epoch [37/50] Batch [900/938] D_loss: 1.3849 G_loss: 0.6863 D_real_acc: 0.5156 D_fake_acc: 0.5781\n",
      "Epoch [37/50] Average - D_loss: 1.3857 G_loss: 0.6977 D_real_acc: 0.4622 D_fake_acc: 0.5605\n",
      "Epoch [38/50] Batch [0/938] D_loss: 1.3861 G_loss: 0.6988 D_real_acc: 0.4844 D_fake_acc: 0.5625\n",
      "Epoch [38/50] Batch [100/938] D_loss: 1.4048 G_loss: 0.7011 D_real_acc: 0.5781 D_fake_acc: 0.4688\n",
      "Epoch [38/50] Batch [200/938] D_loss: 1.3929 G_loss: 0.7067 D_real_acc: 0.3750 D_fake_acc: 0.4688\n",
      "Epoch [38/50] Batch [300/938] D_loss: 1.3837 G_loss: 0.7032 D_real_acc: 0.3594 D_fake_acc: 0.7188\n",
      "Epoch [38/50] Batch [400/938] D_loss: 1.3947 G_loss: 0.6861 D_real_acc: 0.6406 D_fake_acc: 0.2656\n",
      "Epoch [38/50] Batch [500/938] D_loss: 1.3834 G_loss: 0.6920 D_real_acc: 0.2500 D_fake_acc: 0.7812\n",
      "Epoch [38/50] Batch [600/938] D_loss: 1.3821 G_loss: 0.6745 D_real_acc: 0.5781 D_fake_acc: 0.4688\n",
      "Epoch [38/50] Batch [700/938] D_loss: 1.3837 G_loss: 0.6927 D_real_acc: 0.5000 D_fake_acc: 0.5156\n",
      "Epoch [38/50] Batch [800/938] D_loss: 1.3859 G_loss: 0.7020 D_real_acc: 0.5469 D_fake_acc: 0.5625\n",
      "Epoch [38/50] Batch [900/938] D_loss: 1.3940 G_loss: 0.6952 D_real_acc: 0.5156 D_fake_acc: 0.4375\n",
      "Epoch [38/50] Average - D_loss: 1.3859 G_loss: 0.6971 D_real_acc: 0.4842 D_fake_acc: 0.5365\n",
      "Epoch [39/50] Batch [0/938] D_loss: 1.3890 G_loss: 0.6813 D_real_acc: 0.5312 D_fake_acc: 0.3594\n",
      "Epoch [39/50] Batch [100/938] D_loss: 1.3858 G_loss: 0.6947 D_real_acc: 0.5469 D_fake_acc: 0.5156\n",
      "Epoch [39/50] Batch [200/938] D_loss: 1.3764 G_loss: 0.6967 D_real_acc: 0.6562 D_fake_acc: 0.6875\n",
      "Epoch [39/50] Batch [300/938] D_loss: 1.3866 G_loss: 0.6866 D_real_acc: 0.7188 D_fake_acc: 0.3750\n",
      "Epoch [39/50] Batch [400/938] D_loss: 1.3850 G_loss: 0.6916 D_real_acc: 0.5781 D_fake_acc: 0.4688\n",
      "Epoch [39/50] Batch [500/938] D_loss: 1.3841 G_loss: 0.7102 D_real_acc: 0.3438 D_fake_acc: 0.6719\n",
      "Epoch [39/50] Batch [600/938] D_loss: 1.3840 G_loss: 0.7166 D_real_acc: 0.0938 D_fake_acc: 0.9219\n",
      "Epoch [39/50] Batch [700/938] D_loss: 1.3821 G_loss: 0.6942 D_real_acc: 0.6562 D_fake_acc: 0.5312\n",
      "Epoch [39/50] Batch [800/938] D_loss: 1.3763 G_loss: 0.6869 D_real_acc: 0.5625 D_fake_acc: 0.6562\n",
      "Epoch [39/50] Batch [900/938] D_loss: 1.3750 G_loss: 0.7037 D_real_acc: 0.4688 D_fake_acc: 0.7031\n",
      "Epoch [39/50] Average - D_loss: 1.3860 G_loss: 0.6976 D_real_acc: 0.4918 D_fake_acc: 0.5242\n",
      "Epoch [40/50] Batch [0/938] D_loss: 1.3869 G_loss: 0.7016 D_real_acc: 0.5625 D_fake_acc: 0.4375\n",
      "Epoch [40/50] Batch [100/938] D_loss: 1.3876 G_loss: 0.6968 D_real_acc: 0.3906 D_fake_acc: 0.5156\n",
      "Epoch [40/50] Batch [200/938] D_loss: 1.3895 G_loss: 0.6896 D_real_acc: 0.5156 D_fake_acc: 0.4531\n",
      "Epoch [40/50] Batch [300/938] D_loss: 1.3810 G_loss: 0.7036 D_real_acc: 0.5156 D_fake_acc: 0.5938\n",
      "Epoch [40/50] Batch [400/938] D_loss: 1.3905 G_loss: 0.7097 D_real_acc: 0.4375 D_fake_acc: 0.5156\n",
      "Epoch [40/50] Batch [500/938] D_loss: 1.3851 G_loss: 0.6940 D_real_acc: 0.4375 D_fake_acc: 0.5469\n",
      "Epoch [40/50] Batch [600/938] D_loss: 1.3769 G_loss: 0.6732 D_real_acc: 0.7969 D_fake_acc: 0.2969\n",
      "Epoch [40/50] Batch [700/938] D_loss: 1.3764 G_loss: 0.6844 D_real_acc: 0.7812 D_fake_acc: 0.3438\n",
      "Epoch [40/50] Batch [800/938] D_loss: 1.3871 G_loss: 0.7001 D_real_acc: 0.5156 D_fake_acc: 0.5312\n",
      "Epoch [40/50] Batch [900/938] D_loss: 1.3826 G_loss: 0.7096 D_real_acc: 0.3750 D_fake_acc: 0.6719\n",
      "Epoch [40/50] Average - D_loss: 1.3855 G_loss: 0.6979 D_real_acc: 0.4862 D_fake_acc: 0.5371\n",
      "Generated images saved to generated_images/epoch_40.png\n",
      "Epoch [41/50] Batch [0/938] D_loss: 1.3878 G_loss: 0.6773 D_real_acc: 0.7344 D_fake_acc: 0.2656\n",
      "Epoch [41/50] Batch [100/938] D_loss: 1.3836 G_loss: 0.6854 D_real_acc: 0.7188 D_fake_acc: 0.2812\n",
      "Epoch [41/50] Batch [200/938] D_loss: 1.3914 G_loss: 0.6915 D_real_acc: 0.4688 D_fake_acc: 0.4688\n",
      "Epoch [41/50] Batch [300/938] D_loss: 1.3759 G_loss: 0.6891 D_real_acc: 0.7656 D_fake_acc: 0.4219\n",
      "Epoch [41/50] Batch [400/938] D_loss: 1.3797 G_loss: 0.7266 D_real_acc: 0.3438 D_fake_acc: 0.7500\n",
      "Epoch [41/50] Batch [500/938] D_loss: 1.3851 G_loss: 0.7034 D_real_acc: 0.3750 D_fake_acc: 0.6250\n",
      "Epoch [41/50] Batch [600/938] D_loss: 1.3888 G_loss: 0.7240 D_real_acc: 0.0781 D_fake_acc: 0.8594\n",
      "Epoch [41/50] Batch [700/938] D_loss: 1.3700 G_loss: 0.7013 D_real_acc: 0.5469 D_fake_acc: 0.5938\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0002\n",
    "NOISE_DIM = 100\n",
    "NUM_EPOCHS = 50\n",
    "BETA1 = 0.5  # Beta1 for Adam optimizer\n",
    "\n",
    "# Create directory for saving images\n",
    "os.makedirs('generated_images', exist_ok=True)\n",
    "\n",
    "# Data preprocessing and loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load MNIST dataset from local files\n",
    "# First, let's try to use the local data without downloading\n",
    "try:\n",
    "    print(\"Attempting to load local MNIST dataset...\")\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
    "    print(f\"Successfully loaded local dataset: {len(train_dataset)} training samples\")\n",
    "except:\n",
    "    print(\"Local dataset not found in expected format. Checking file structure...\")\n",
    "    \n",
    "    # Check if the raw files exist\n",
    "    import os\n",
    "    data_files = [\n",
    "        './data/train-images.idx3-ubyte',\n",
    "        './data/train-labels.idx1-ubyte',\n",
    "        './data/t10k-images.idx3-ubyte',\n",
    "        './data/t10k-labels.idx1-ubyte'\n",
    "    ]\n",
    "    \n",
    "    files_exist = all(os.path.exists(f) for f in data_files)\n",
    "    \n",
    "    if files_exist:\n",
    "        print(\"Raw MNIST files found! Creating dataset structure...\")\n",
    "        \n",
    "        # Create the proper directory structure for torchvision\n",
    "        processed_dir = './data/MNIST/processed'\n",
    "        raw_dir = './data/MNIST/raw'\n",
    "        os.makedirs(processed_dir, exist_ok=True)\n",
    "        os.makedirs(raw_dir, exist_ok=True)\n",
    "        \n",
    "        # Copy files to expected locations if they don't exist\n",
    "        import shutil\n",
    "        file_mapping = {\n",
    "            './data/train-images.idx3-ubyte': './data/MNIST/raw/train-images-idx3-ubyte',\n",
    "            './data/train-labels.idx1-ubyte': './data/MNIST/raw/train-labels-idx1-ubyte',\n",
    "            './data/t10k-images.idx3-ubyte': './data/MNIST/raw/t10k-images-idx3-ubyte',\n",
    "            './data/t10k-labels.idx1-ubyte': './data/MNIST/raw/t10k-labels-idx1-ubyte'\n",
    "        }\n",
    "        \n",
    "        for src, dst in file_mapping.items():\n",
    "            if os.path.exists(src) and not os.path.exists(dst):\n",
    "                shutil.copy2(src, dst)\n",
    "                print(f\"Copied {src} to {dst}\")\n",
    "        \n",
    "        # Now try loading again\n",
    "        try:\n",
    "            train_dataset = datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
    "            print(f\"Successfully loaded restructured local dataset: {len(train_dataset)} training samples\")\n",
    "        except:\n",
    "            print(\"Still having issues with local files. Using alternative loading method...\")\n",
    "            \n",
    "            # Alternative: Custom dataset loader for your file structure\n",
    "            class LocalMNIST(torch.utils.data.Dataset):\n",
    "                def __init__(self, images_file, labels_file, transform=None):\n",
    "                    import struct\n",
    "                    \n",
    "                    # Load images\n",
    "                    with open(images_file, 'rb') as f:\n",
    "                        magic, num_images, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "                        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "                        images = images.reshape(num_images, rows, cols)\n",
    "                    \n",
    "                    # Load labels\n",
    "                    with open(labels_file, 'rb') as f:\n",
    "                        magic, num_labels = struct.unpack('>II', f.read(8))\n",
    "                        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "                    \n",
    "                    self.images = images\n",
    "                    self.labels = labels\n",
    "                    self.transform = transform\n",
    "                \n",
    "                def __len__(self):\n",
    "                    return len(self.images)\n",
    "                \n",
    "                def __getitem__(self, idx):\n",
    "                    image = self.images[idx]\n",
    "                    label = self.labels[idx]\n",
    "                    \n",
    "                    # Convert to PIL Image for transform compatibility\n",
    "                    from PIL import Image\n",
    "                    image = Image.fromarray(image, mode='L')\n",
    "                    \n",
    "                    if self.transform:\n",
    "                        image = self.transform(image)\n",
    "                    \n",
    "                    return image, label\n",
    "            \n",
    "            # Use custom loader\n",
    "            print(\"Using custom dataset loader...\")\n",
    "            train_dataset = LocalMNIST('./data/train-images.idx3-ubyte', './data/train-labels.idx1-ubyte', transform=transform)\n",
    "            print(f\"Custom dataset loaded: {len(train_dataset)} training samples\")\n",
    "    else:\n",
    "        print(\"Raw MNIST files not found. Downloading dataset...\")\n",
    "        train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Dataset loaded: {len(train_dataset)} training samples\")\n",
    "\n",
    "# Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # First layer\n",
    "            nn.Linear(noise_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Second layer\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Third layer\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(1024, 784),  # 28*28 = 784\n",
    "            nn.Tanh()  # Output in [-1, 1] to match normalized data\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # First layer\n",
    "            nn.Linear(784, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Second layer\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Third layer\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()  # Output probability [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize networks\n",
    "generator = Generator(NOISE_DIM).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "print(\"Networks initialized\")\n",
    "print(f\"Generator parameters: {sum(p.numel() for p in generator.parameters())}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in discriminator.parameters())}\")\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(BETA1, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(BETA1, 0.999))\n",
    "\n",
    "# Labels for real and fake samples\n",
    "real_label = 1.0\n",
    "fake_label = 0.0\n",
    "\n",
    "# Fixed noise for consistent visualization\n",
    "fixed_noise = torch.randn(64, NOISE_DIM, device=device)\n",
    "\n",
    "# Training history\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "D_real_accuracy = []\n",
    "D_fake_accuracy = []\n",
    "\n",
    "def save_generated_images(generator, epoch, noise, save_path):\n",
    "    \"\"\"Save a grid of generated images\"\"\"\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        fake_images = generator(noise).detach().cpu()\n",
    "        fake_images = fake_images.view(-1, 1, 28, 28)\n",
    "        fake_images = (fake_images + 1) / 2  # Denormalize from [-1,1] to [0,1]\n",
    "        \n",
    "        # Create a grid of images\n",
    "        fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "        fig.suptitle(f'Generated Images - Epoch {epoch}', fontsize=16)\n",
    "        \n",
    "        for i in range(64):\n",
    "            row, col = i // 8, i % 8\n",
    "            axes[row, col].imshow(fake_images[i, 0], cmap='gray')\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    generator.train()\n",
    "\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    \"\"\"Calculate accuracy for discriminator predictions\"\"\"\n",
    "    predicted = (predictions > 0.5).float()\n",
    "    correct = (predicted == targets).float().sum()\n",
    "    return correct / targets.size(0)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_G_loss = 0.0\n",
    "    epoch_D_loss = 0.0\n",
    "    epoch_D_real_acc = 0.0\n",
    "    epoch_D_fake_acc = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for i, (real_images, _) in enumerate(train_loader):\n",
    "        batch_size = real_images.size(0)\n",
    "        real_images = real_images.to(device)\n",
    "        \n",
    "        # Create labels\n",
    "        real_labels = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
    "        fake_labels = torch.full((batch_size,), fake_label, dtype=torch.float, device=device)\n",
    "        \n",
    "        # ============================================\n",
    "        # Train Discriminator: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        # ============================================\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        # Train with real images\n",
    "        real_images_flat = real_images.view(batch_size, -1)\n",
    "        output_real = discriminator(real_images_flat).view(-1)\n",
    "        loss_D_real = criterion(output_real, real_labels)\n",
    "        \n",
    "        # Train with fake images\n",
    "        noise = torch.randn(batch_size, NOISE_DIM, device=device)\n",
    "        fake_images = generator(noise)\n",
    "        output_fake = discriminator(fake_images.detach()).view(-1)\n",
    "        loss_D_fake = criterion(output_fake, fake_labels)\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        loss_D = loss_D_real + loss_D_fake\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Calculate discriminator accuracy\n",
    "        acc_real = calculate_accuracy(output_real, real_labels)\n",
    "        acc_fake = calculate_accuracy(output_fake, fake_labels)\n",
    "        \n",
    "        # ============================================\n",
    "        # Train Generator: maximize log(D(G(z)))\n",
    "        # ============================================\n",
    "        generator.zero_grad()\n",
    "        \n",
    "        # Generate fake images and get discriminator's opinion\n",
    "        output_fake_for_G = discriminator(fake_images).view(-1)\n",
    "        # Generator wants discriminator to think fake images are real\n",
    "        loss_G = criterion(output_fake_for_G, real_labels)\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Accumulate losses and accuracies\n",
    "        epoch_G_loss += loss_G.item()\n",
    "        epoch_D_loss += loss_D.item()\n",
    "        epoch_D_real_acc += acc_real.item()\n",
    "        epoch_D_fake_acc += acc_fake.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{NUM_EPOCHS}] Batch [{i}/{len(train_loader)}] '\n",
    "                  f'D_loss: {loss_D.item():.4f} G_loss: {loss_G.item():.4f} '\n",
    "                  f'D_real_acc: {acc_real.item():.4f} D_fake_acc: {acc_fake.item():.4f}')\n",
    "    \n",
    "    # Calculate average losses and accuracies for the epoch\n",
    "    avg_G_loss = epoch_G_loss / num_batches\n",
    "    avg_D_loss = epoch_D_loss / num_batches\n",
    "    avg_D_real_acc = epoch_D_real_acc / num_batches\n",
    "    avg_D_fake_acc = epoch_D_fake_acc / num_batches\n",
    "    \n",
    "    # Store losses for plotting\n",
    "    G_losses.append(avg_G_loss)\n",
    "    D_losses.append(avg_D_loss)\n",
    "    D_real_accuracy.append(avg_D_real_acc)\n",
    "    D_fake_accuracy.append(avg_D_fake_acc)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}] Average - D_loss: {avg_D_loss:.4f} '\n",
    "          f'G_loss: {avg_G_loss:.4f} D_real_acc: {avg_D_real_acc:.4f} '\n",
    "          f'D_fake_acc: {avg_D_fake_acc:.4f}')\n",
    "    \n",
    "    # Save generated images every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        save_path = f'generated_images/epoch_{epoch+1:02d}.png'\n",
    "        save_generated_images(generator, epoch+1, fixed_noise, save_path)\n",
    "        print(f'Generated images saved to {save_path}')\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# ============================================\n",
    "# Plotting Results\n",
    "# ============================================\n",
    "\n",
    "# Plot training losses\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(G_losses, label='Generator Loss', color='blue')\n",
    "plt.plot(D_losses, label='Discriminator Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Losses')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Discriminator accuracy on real images\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(D_real_accuracy, label='Real Images', color='green')\n",
    "plt.plot(D_fake_accuracy, label='Fake Images', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Discriminator Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Combined view of discriminator performance\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot([(1 - acc) for acc in D_fake_accuracy], label='Generator Success Rate', color='purple')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Generator Success Rate\\n(1 - D_fake_accuracy)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# Generate Final Results\n",
    "# ============================================\n",
    "\n",
    "# Generate final batch of images\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    final_noise = torch.randn(64, NOISE_DIM, device=device)\n",
    "    final_fake_images = generator(final_noise).detach().cpu()\n",
    "    final_fake_images = final_fake_images.view(-1, 1, 28, 28)\n",
    "    final_fake_images = (final_fake_images + 1) / 2  # Denormalize\n",
    "\n",
    "# Display final results\n",
    "fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
    "fig.suptitle('Final Generated MNIST Digits', fontsize=16)\n",
    "\n",
    "for i in range(64):\n",
    "    row, col = i // 8, i % 8\n",
    "    axes[row, col].imshow(final_fake_images[i, 0], cmap='gray')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_generated_digits.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# Analysis and Metrics\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nFinal Losses:\")\n",
    "print(f\"Generator Loss: {G_losses[-1]:.4f}\")\n",
    "print(f\"Discriminator Loss: {D_losses[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Accuracies:\")\n",
    "print(f\"Discriminator on Real Images: {D_real_accuracy[-1]:.4f}\")\n",
    "print(f\"Discriminator on Fake Images: {D_fake_accuracy[-1]:.4f}\")\n",
    "print(f\"Generator Success Rate: {1 - D_fake_accuracy[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining Stability Metrics:\")\n",
    "loss_variance_G = np.var(G_losses[-10:])  # Variance in last 10 epochs\n",
    "loss_variance_D = np.var(D_losses[-10:])\n",
    "print(f\"Generator Loss Variance (last 10 epochs): {loss_variance_G:.6f}\")\n",
    "print(f\"Discriminator Loss Variance (last 10 epochs): {loss_variance_D:.6f}\")\n",
    "\n",
    "# Check for mode collapse indicators\n",
    "print(f\"\\nMode Collapse Indicators:\")\n",
    "print(f\"Discriminator accuracy on fake images should be around 0.5 for balanced training\")\n",
    "print(f\"Current D_fake accuracy: {D_fake_accuracy[-1]:.4f}\")\n",
    "if D_fake_accuracy[-1] < 0.3:\n",
    "    print(\"⚠️  Generator may be fooling discriminator too easily\")\n",
    "elif D_fake_accuracy[-1] > 0.7:\n",
    "    print(\"⚠️  Discriminator may be too strong, hampering generator learning\")\n",
    "else:\n",
    "    print(\"✅ Training appears balanced\")\n",
    "\n",
    "print(f\"\\nNetwork Architecture Summary:\")\n",
    "print(f\"Generator: {NOISE_DIM} → 256 → 512 → 1024 → 784\")\n",
    "print(f\"Discriminator: 784 → 512 → 256 → 128 → 1\")\n",
    "\n",
    "# Save training history\n",
    "training_history = {\n",
    "    'generator_losses': G_losses,\n",
    "    'discriminator_losses': D_losses,\n",
    "    'discriminator_real_accuracy': D_real_accuracy,\n",
    "    'discriminator_fake_accuracy': D_fake_accuracy\n",
    "}\n",
    "\n",
    "torch.save(training_history, 'training_history.pt')\n",
    "torch.save(generator.state_dict(), 'generator_final.pt')\n",
    "torch.save(discriminator.state_dict(), 'discriminator_final.pt')\n",
    "\n",
    "print(f\"\\nModels and training history saved!\")\n",
    "print(f\"Generated images saved in 'generated_images/' directory\")\n",
    "print(f\"Training curves saved as 'training_curves.png'\")\n",
    "print(f\"Final results saved as 'final_generated_digits.png'\")\n",
    "\n",
    "# ============================================\n",
    "# Advanced Analysis Functions\n",
    "# ============================================\n",
    "\n",
    "def analyze_digit_diversity():\n",
    "    \"\"\"Analyze the diversity of generated digits\"\"\"\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generate a large batch for analysis\n",
    "        noise = torch.randn(1000, NOISE_DIM, device=device)\n",
    "        generated = generator(noise).detach().cpu()\n",
    "        generated = generated.view(-1, 28, 28)\n",
    "        generated = (generated + 1) / 2  # Denormalize\n",
    "        \n",
    "        # Calculate some basic diversity metrics\n",
    "        pixel_variance = torch.var(generated, dim=0).mean().item()\n",
    "        print(f\"\\nDiversity Analysis:\")\n",
    "        print(f\"Average pixel variance across generated samples: {pixel_variance:.6f}\")\n",
    "        \n",
    "        return generated\n",
    "\n",
    "def interpolation_analysis():\n",
    "    \"\"\"Generate interpolation between two noise vectors\"\"\"\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        # Create two random noise vectors\n",
    "        z1 = torch.randn(1, NOISE_DIM, device=device)\n",
    "        z2 = torch.randn(1, NOISE_DIM, device=device)\n",
    "        \n",
    "        # Create interpolation\n",
    "        alphas = torch.linspace(0, 1, 10)\n",
    "        interpolated_images = []\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            z_interp = alpha * z1 + (1 - alpha) * z2\n",
    "            img = generator(z_interp).detach().cpu()\n",
    "            img = img.view(28, 28)\n",
    "            img = (img + 1) / 2  # Denormalize\n",
    "            interpolated_images.append(img)\n",
    "        \n",
    "        # Plot interpolation\n",
    "        fig, axes = plt.subplots(1, 10, figsize=(20, 2))\n",
    "        fig.suptitle('Noise Vector Interpolation', fontsize=14)\n",
    "        \n",
    "        for i, img in enumerate(interpolated_images):\n",
    "            axes[i].imshow(img, cmap='gray')\n",
    "            axes[i].set_title(f'α={alphas[i]:.1f}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('interpolation_analysis.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Run additional analyses\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADDITIONAL ANALYSES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analyze_digit_diversity()\n",
    "interpolation_analysis()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXERCISE COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
